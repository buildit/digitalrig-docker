= K8S version of Jenkins-based CD pipeline.

== Prerequisites

* Kubernetes 1.5.x cluster or Minikube 0.16.x
** In case of Minikube - you should install virtualbox first
*** `brew cask install virtualbox`
*** `brew cask install minikube`
** As a workaround for https://github.com/kubernetes/minikube/issues/1056[#1056] start minikube using old image (`minikube start --iso-url https://storage.googleapis.com/minikube/minikube-0.7.iso`)
*** should be fixed in 1.6.0+
** Pass port range parameter to allow Minikube to use lower ports `minikube start --extra-config=apiserver.GenericServerRunOptions.ServiceNodePortRange=80-32000`
* https://kubernetes.io/docs/user-guide/prereqs/[kubectl cli]
** `brew install kubectl`
* https://github.com/kubernetes/helm[Helm] 2.x
** `brew install kubernetes-helm`
* https://www.terraform.io/downloads.html[Terraform] 0.8.x+
** `brew install terraform`
* wget
** `brew install wget`
* Digitalrig repos cloned at the same parent directory
** https://github.com/buildit/digitalrig-docker[this repo] (`git clone https://github.com/buildit/digitalrig-docker.git`)
** https://github.com/electroma/charts/[fork] of `kubernetes/charts` (`git clone https://github.com/electroma/charts.git`)
** https://github.com/buildit/digitalrig-acceptance-tests[digitalrig-acceptance-tests] (`git clone https://github.com/buildit/digitalrig-acceptance-tests.git`)

See link:ec2/README.adoc[EC2 cluster configuration steps]

== Installation

. Make sure you're connected to the cluster: `kubectl get nodes` should return list of nodes and masters
. Open terminal and go to the directory with copy of this repo
. Generate VPN keys (run `keys/generate_keys.sh` and use output to create client VPN configuration and populate `openvpn.yaml` by creating a copy of `openvpn.yaml.tpl`)
. Configure chart parameters (do not forget to create configuration from tpl files!)
.. `helm/vars/` for Minikube
.. `helm/ec2_vars` for EC2
. Execute commands to install RIG components
.. ensure working directory is `helm`
.. from `rig_install.sh` for Minikube
.. from `ec2_rig_install.sh` for EC2
.. *Important*: make sure you're using correct cluster (`kubectl config get-contexts`)!
. Create name records for static resources
.. On EC2
... create vpn.{{public.domain}} CNAME
... create internal CNAME *.riglet pointing to nginx-int ELB
... create public CNAME *.apps.{{public.domain}} pointing to nginx-pub ELB
.. On Minikube
... https://gist.github.com/eloypnd/5efc3b590e7c738630fdcf0c10b68072[install dnsmasq] and configure resolver for `.local` domain + wildcard DNS record pointing to `minikube ip` output
... More lightweight alternative is to add explicit host-record to make name-based routing work
----
echo "`minikube ip` nginx.kube.local jenkins.kube.local" >> /etc/hosts`
----

== Minikube hacks and tips

. Don't forget to add `/etc/hosts` record after deploying new application (ingress controller uses host name to route http requests)
. If you're running on minikube and want to VPN into POD network - you may need to make sure network is configured correctly
.. `minikube ssh` and check `sudo udhcp` - it should give you the understanding of b2d dns setup
.. Make sure corresponding route is in place
.. In case you want to make changes to DNS configuration most likely you gonna need to restart kube-dns
----
kubectl delete pod -n kube-system `kubectl get pods -n kube-system -l "k8s-app=kube-dns" --template "{{ range .items }}{{.metadata.name}} {{end}}"`
----
. You may want to give minikube more resources (i.e. `minikube start --cpus 2 --memory 4096`)
. Make sure you're running master or version 0.17+ to have real PVs (see https://github.com/kubernetes/minikube/issues/1103)
.. In case you're running master you need to build master iso image (see https://github.com/tvon/minikube/commit/df08ad854dacbdc47ffa3012a027a632dbf325fd)
   and configure start ISO `minikube start --cpus 2 --memory 4096 --iso-url=file:///$GOPATH/k8s.io/minikube/out/buildroot/output/images/rootfs.iso9660`

== Usage on Minikube

. Jenkins is available on http://jenkins.kube.local
. NGINX Internal web console is available on http://nginx.kube.local:NNNNN (where NNNNN is the node port `monitoring` of `nginx-int`)
. NGINX Public web console is available on http://nginx.kube.local:NNNNN (where NNNNN is the node port `monitoring` of `nginx-pub`)

== Usage on EC2

. K8S cluster is deployed in VPC and is not accessible from the internet
. Public applications will be available under `.apps.[cluster.domain]`
. Public NGINX web console is available on http://nginx.apps.{{public.domain}}
. To access internal apps you need to login into VPN first
.. Jenkins is available on http://jenkins.riglet
.. Internal NGINX web console is available on http://nginx-int-nginx-lego-monitoring.default.svc.cluster.local:18080/nginx_status/

== Principles

* We're using NGINX as k8s ingress controller
* There are two ingress controllers: private (available within VPC) and public (exposed using external ELB)
* All resources are available on private ingress controller
* Ingress objects in `public` namespace are available on both internal and public ingress controller

== Example job for Minikube

*Note*: change mount path for `/var/gitrepo` to match your setup (you may need to clone https://github.com/buildit/digitalrig-acceptance-tests[digitalrig-acceptance-tests] first)

[source,groovy]
----
podTemplate(label: 'nodeapp',
            containers: [
                containerTemplate(name: 'nodejs-builder', image: 'builditdigital/node-builder', ttyEnabled: true, command: 'cat', privileged: true),
                containerTemplate(name: 'docker', image: 'docker:1.11', ttyEnabled: true, command: 'cat'),
                containerTemplate(name: 'kubectl', image: 'lachlanevenson/k8s-kubectl', ttyEnabled: true, command: 'cat')],
            volumes: [
                hostPathVolume(mountPath: '/var/gitrepo', hostPath: '/Users/romansafronov/dev/projects/digitalrig-acceptance-tests'),
                hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock')]) {
    node('nodeapp') {
        def nextVersion = new Date().time as String
        container('nodejs-builder') {
            stage('Checkout') {
                git(url: 'file:///var/gitrepo')
            }
            stage('Build') {
                sh 'cd ./src/test/apps/node-docker && npm install && npm run dist'
            }
        }

        container('docker') {
            stage('Package') {
                sh "cd ./src/test/apps/node-docker && docker build -t my-environment:${nextVersion} ."
            }
        }
        container('kubectl') {
            stage('Deploy') {
                sh "kubectl get deploy -o name | grep sample-node-app || kubectl create -f src/test/apps/node-docker/kubernetes/sample.yml"
                sh "kubectl set image deployment/sample-node-app-deployment sample-node-app=my-environment:$nextVersion"
                sh 'kubectl rollout status deployment/sample-node-app-deployment'
            }
        }
        container('nodejs-builder') {
            stage('e2e test') {
                //nasty workaround for temporary chrome socket issue (can't use remote mount for it)
                sh "mkdir /tmp/wscopy && cd ./src/test/apps/node-docker && ls -1 | xargs -I '{}'  ln -s `pwd`/{} /tmp/wscopy/{}"
                sh "cd /tmp/wscopy && URL=http://sample-node-app-svc# xvfb-run --server-args='-screen 0, 1024x768x16'  npm run test:e2e"
            }
        }

        // TODO: ROLLBACK RELEASE ON FAILURE??
    }
}
----

== Example job for EC2

*Note*: you need to change `region` variable to match your AWS region and create ECR repository (set `repoName` variable in the following job definition)

[source,groovy]
----
podTemplate(label: 'nodeapp',
            containers: [
                containerTemplate(name: 'nodejs-builder', image: 'builditdigital/node-builder', ttyEnabled: true, command: 'cat', privileged: true),
                containerTemplate(name: 'aws', image: 'cgswong/aws', ttyEnabled: true, command: 'cat'),
                containerTemplate(name: 'docker', image: 'docker:1.11', ttyEnabled: true, command: 'cat'),
                containerTemplate(name: 'kubectl', image: 'lachlanevenson/k8s-kubectl', ttyEnabled: true, command: 'cat')],
            volumes: [
                hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock')]) {
    node('nodeapp') {
        def nextVersion = new Date().time as String
        def repoName = 'rsafronov-k8s-sample-app'
        def region = 'us-east-1'
        container('nodejs-builder') {
            stage('Checkout') {
                git(url: 'https://github.com/buildit/digitalrig-acceptance-tests.git')
            }
            stage('Build') {
                sh 'cd ./src/test/apps/node-docker && ls -l && npm install && npm run dist'
            }
        }

        def loginCmd = ''
        container('aws') {
            loginCmd = sh script: "aws ecr get-login --region=${region}", returnStdout: true
        }

        container('docker') {
            stage('Package') {
                sh loginCmd
                sh "docker build -t ${repoName}:${nextVersion} ./src/test/apps/node-docker"
                sh "docker tag ${repoName}:${nextVersion} 006393696278.dkr.ecr.${region}.amazonaws.com/${repoName}:${nextVersion}"
                sh "docker push 006393696278.dkr.ecr.${region}.amazonaws.com/${repoName}:${nextVersion}"
            }
        }
        container('kubectl') {
            stage('Deploy') {
                sh "kubectl get deploy -o name | grep sample-node-app || kubectl create -f src/test/apps/node-docker/kubernetes/sample.yml"
                sh "kubectl set image deployment/sample-node-app-deployment sample-node-app=006393696278.dkr.ecr.${region}.amazonaws.com/${repoName}:${nextVersion}"
                sh 'kubectl rollout status deployment/sample-node-app-deployment'
            }
        }
        container('nodejs-builder') {
            stage('e2e test') {
                //nasty workaround for temporary chrome socket issue (can't use remote mount for it)
                sh "mkdir /tmp/wscopy && cd ./src/test/apps/node-docker && ls -1 | xargs -I '{}'  ln -s `pwd`/{} /tmp/wscopy/{}"
                sh "cd /tmp/wscopy && URL=http://sample-node-app-svc# xvfb-run --server-args='-screen 0, 1024x768x16'  npm run test:e2e"
            }
        }
   }
}
----

== Next steps

* Read about link:security.adoc[security perimiter]
* Install https://github.com/buildit/heimdall/tree/master/k8s[Heimdall] to enable access control for other components
* Build some applications
** https://github.com/buildit/Eolas/tree/master/k8s[Eolas]
** https://github.com/buildit/Synapse/tree/master/k8s[Synapse]
** Twig-api
** Twig

== Contributing to charts

We're using https://github.com/electroma/charts/[fork] of https://github.com/kubernetes/charts[kubernetes/charts].

In case you need to make change in an existing chart or create new public chart:

. Create feature branch from `baseline` branch
. Make and test your changes
. Create PR to upstream (there are some https://github.com/electroma/charts/blob/master/CONTRIBUTING.md[rules])
. Merge your changes to `master` branch to make it available
. Once your PR is merged
.. Sync `upstream` branch from `kubernetes/charts`
.. Merge `upstream` to `master`

== Publishing public Docker images

If you want to update or create new public Docker image - please do it under one of our organisations:

* digitalrig
* builditdigital

TBD...

* Image build automation
* Versioning approach

== TODOs

* Migrate sample app to helm
* Better defaults for VPN keys generation (CA, client and server name)
* Better automation of rig components installation (create composite chart?)
* Automate sample job deployment
